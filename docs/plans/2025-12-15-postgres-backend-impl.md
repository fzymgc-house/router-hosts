# PostgreSQL Backend Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement PostgreSQL storage backend using sqlx with connection pooling.

**Architecture:** True async PostgreSQL backend using sqlx's PgPool. Window functions with IGNORE NULLS like DuckDB. Testcontainers for testing.

**Tech Stack:** sqlx 0.8, tokio, testcontainers-modules

---

## Task 1: Add Dependencies

**Files:**
- Modify: `crates/router-hosts-storage/Cargo.toml`

**Step 1: Add sqlx dependency and postgres feature**

```toml
# Add to [dependencies] section after rusqlite line:
sqlx = { version = "0.8", features = ["runtime-tokio", "postgres", "tls-rustls"], optional = true }

# Add to [features] section:
postgres = ["dep:sqlx"]

# Add to [dev-dependencies] section:
testcontainers = "0.23"
testcontainers-modules = { version = "0.11", features = ["postgres"] }
```

**Step 2: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Compiles with warnings about unused deps (that's fine)

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/Cargo.toml
git commit -m "build(storage): add sqlx and testcontainers deps for postgres

Add sqlx 0.8 with postgres/rustls features behind 'postgres' feature flag.
Add testcontainers for integration testing.

Refs #113"
```

---

## Task 2: Create Module Structure

**Files:**
- Create: `crates/router-hosts-storage/src/backends/postgres/mod.rs`
- Modify: `crates/router-hosts-storage/src/backends/mod.rs`

**Step 1: Create postgres mod.rs skeleton**

```rust
//! PostgreSQL storage backend implementation
//!
//! This module provides event-sourced storage using PostgreSQL with sqlx.
//! PostgreSQL is ideal for multi-instance and cloud deployments where
//! horizontal scaling and high availability are required.
//!
//! # Architecture
//!
//! - **schema**: Table definitions (CREATE TABLE IF NOT EXISTS)
//! - **event_store**: Event sourcing write side (append-only events)
//! - **snapshot_store**: /etc/hosts versioning and snapshots
//! - **projection**: CQRS read side (window functions with IGNORE NULLS)
//!
//! # Connection Pooling
//!
//! Uses sqlx's PgPool for connection management:
//! - min_connections: 1 (keep warm)
//! - max_connections: 10 (prevent overwhelming)
//! - acquire_timeout: 30s (match gRPC timeout)
//! - idle_timeout: 10min (release unused)
//!
//! Pool settings can be overridden via connection string query params.
//!
//! # Differences from SQLite/DuckDB
//!
//! - True async (no spawn_blocking wrappers)
//! - Supports IGNORE NULLS in window functions (like DuckDB)
//! - Connection pooling for concurrent access
//! - Standard PostgreSQL SSL via sslmode parameter

use async_trait::async_trait;
use chrono::{DateTime, Utc};
use sqlx::postgres::{PgPool, PgPoolOptions};
use std::time::Duration;
use ulid::Ulid;

use crate::error::StorageError;
use crate::traits::{EventStore, HostProjection, SnapshotStore, Storage};
use crate::types::{EventEnvelope, HostEntry, HostFilter, Snapshot, SnapshotId, SnapshotMetadata};

mod event_store;
mod projection;
mod schema;
mod snapshot_store;

pub use schema::initialize_schema;

/// PostgreSQL storage backend
///
/// Provides event-sourced storage using PostgreSQL with connection pooling.
/// All operations are truly async using sqlx.
///
/// # Examples
///
/// ```no_run
/// use router_hosts_storage::backends::postgres::PostgresStorage;
/// use router_hosts_storage::Storage;
///
/// # async fn example() -> Result<(), Box<dyn std::error::Error>> {
/// let storage = PostgresStorage::new("postgres://user:pass@localhost/db").await?;
/// storage.initialize().await?;
/// # Ok(())
/// # }
/// ```
pub struct PostgresStorage {
    pool: PgPool,
}

impl PostgresStorage {
    /// Create a new PostgreSQL storage backend
    ///
    /// # Arguments
    ///
    /// * `url` - PostgreSQL connection URL
    ///   - `postgres://user:pass@host:5432/dbname`
    ///   - `postgres://host/db?sslmode=require`
    ///   - `postgres://host/db?max_connections=20`
    ///
    /// # Pool Configuration
    ///
    /// Default pool settings (overridable via URL query params):
    /// - min_connections: 1
    /// - max_connections: 10
    /// - acquire_timeout: 30s
    /// - idle_timeout: 10min
    ///
    /// # Errors
    ///
    /// Returns `StorageError::Connection` if pool creation fails.
    pub async fn new(url: &str) -> Result<Self, StorageError> {
        let pool = PgPoolOptions::new()
            .min_connections(1)
            .max_connections(10)
            .acquire_timeout(Duration::from_secs(30))
            .idle_timeout(Duration::from_secs(600))
            .connect(url)
            .await
            .map_err(|e| StorageError::connection("failed to create PostgreSQL pool", e))?;

        Ok(Self { pool })
    }

    /// Get a reference to the connection pool for internal use
    fn pool(&self) -> &PgPool {
        &self.pool
    }
}
```

**Step 2: Update backends/mod.rs**

Add after the sqlite module:

```rust
#[cfg(feature = "postgres")]
pub mod postgres;
```

**Step 3: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Errors about missing schema, event_store, etc. modules (expected)

**Step 4: Commit**

```bash
git add crates/router-hosts-storage/src/backends/postgres/mod.rs
git add crates/router-hosts-storage/src/backends/mod.rs
git commit -m "feat(storage): add PostgresStorage struct skeleton

Add PostgresStorage with PgPool connection pooling.
Pool defaults: min=1, max=10, acquire=30s, idle=10min.

Refs #113"
```

---

## Task 3: Implement Schema Initialization

**Files:**
- Create: `crates/router-hosts-storage/src/backends/postgres/schema.rs`

**Step 1: Write schema.rs**

```rust
//! Database schema definitions for PostgreSQL
//!
//! Creates tables, indexes, and views for event-sourced storage.
//! Uses CREATE TABLE IF NOT EXISTS for idempotent initialization.

use super::PostgresStorage;
use crate::error::StorageError;

/// Initialize the PostgreSQL schema for event-sourced storage
///
/// Creates all tables, indexes, and views required for CQRS event sourcing.
/// Safe to call multiple times (uses IF NOT EXISTS).
pub async fn initialize_schema(storage: &PostgresStorage) -> Result<(), StorageError> {
    let pool = storage.pool();

    // Event store - append-only immutable log of all domain events
    sqlx::query(
        r#"
        CREATE TABLE IF NOT EXISTS host_events (
            event_id TEXT PRIMARY KEY,
            aggregate_id TEXT NOT NULL,
            event_type TEXT NOT NULL,
            event_version TEXT NOT NULL,
            ip_address TEXT,
            hostname TEXT,
            comment TEXT,
            tags TEXT,
            event_timestamp TIMESTAMPTZ NOT NULL,
            metadata TEXT NOT NULL,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            created_by TEXT,
            expected_version TEXT,
            UNIQUE(aggregate_id, event_version)
        )
        "#,
    )
    .execute(pool)
    .await
    .map_err(|e| StorageError::migration("failed to create host_events table", e))?;

    // Index for fast event replay by aggregate
    sqlx::query(
        "CREATE INDEX IF NOT EXISTS idx_events_aggregate ON host_events(aggregate_id, event_version)",
    )
    .execute(pool)
    .await
    .map_err(|e| StorageError::migration("failed to create aggregate index", e))?;

    // Index for temporal queries
    sqlx::query("CREATE INDEX IF NOT EXISTS idx_events_time ON host_events(created_at)")
        .execute(pool)
        .await
        .map_err(|e| StorageError::migration("failed to create temporal index", e))?;

    // Drop existing view to recreate (views can't use IF NOT EXISTS with OR REPLACE in all PG versions)
    sqlx::query("DROP VIEW IF EXISTS host_entries_current")
        .execute(pool)
        .await
        .map_err(|e| StorageError::migration("failed to drop existing view", e))?;

    // Current hosts view using window functions with IGNORE NULLS
    sqlx::query(
        r#"
        CREATE VIEW host_entries_current AS
        WITH windowed AS (
            SELECT
                aggregate_id,
                event_version,
                event_type,
                LAST_VALUE(ip_address) IGNORE NULLS OVER w as ip_address,
                LAST_VALUE(hostname) IGNORE NULLS OVER w as hostname,
                LAST_VALUE(comment) IGNORE NULLS OVER w as comment,
                LAST_VALUE(tags) IGNORE NULLS OVER w as tags,
                FIRST_VALUE(event_timestamp) OVER w as created_at,
                LAST_VALUE(created_at) OVER w as updated_at,
                LAST_VALUE(event_type) OVER w as latest_event_type,
                ROW_NUMBER() OVER (PARTITION BY aggregate_id ORDER BY event_version DESC) as rn
            FROM host_events
            WINDOW w AS (PARTITION BY aggregate_id ORDER BY event_version
                         ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
        )
        SELECT
            aggregate_id as id,
            ip_address,
            hostname,
            comment,
            tags,
            created_at,
            updated_at,
            event_version
        FROM windowed
        WHERE rn = 1 AND latest_event_type != 'HostDeleted'
        "#,
    )
    .execute(pool)
    .await
    .map_err(|e| StorageError::migration("failed to create current hosts view", e))?;

    // Drop existing history view
    sqlx::query("DROP VIEW IF EXISTS host_entries_history")
        .execute(pool)
        .await
        .map_err(|e| StorageError::migration("failed to drop existing history view", e))?;

    // History view
    sqlx::query(
        r#"
        CREATE VIEW host_entries_history AS
        SELECT
            event_id,
            aggregate_id,
            event_type,
            event_version,
            ip_address,
            hostname,
            metadata,
            event_timestamp,
            created_at
        FROM host_events
        ORDER BY aggregate_id, event_version
        "#,
    )
    .execute(pool)
    .await
    .map_err(|e| StorageError::migration("failed to create history view", e))?;

    // Snapshots table
    sqlx::query(
        r#"
        CREATE TABLE IF NOT EXISTS snapshots (
            snapshot_id TEXT PRIMARY KEY,
            created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
            hosts_content TEXT NOT NULL,
            entry_count INTEGER NOT NULL,
            trigger TEXT NOT NULL,
            name TEXT,
            event_log_position BIGINT
        )
        "#,
    )
    .execute(pool)
    .await
    .map_err(|e| StorageError::migration("failed to create snapshots table", e))?;

    // Index for snapshot queries by time
    sqlx::query("CREATE INDEX IF NOT EXISTS idx_snapshots_created ON snapshots(created_at DESC)")
        .execute(pool)
        .await
        .map_err(|e| StorageError::migration("failed to create snapshot index", e))?;

    Ok(())
}

#[cfg(test)]
mod tests {
    // Tests will use testcontainers in the integration test file
}
```

**Step 2: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Still errors about missing event_store, etc.

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/src/backends/postgres/schema.rs
git commit -m "feat(storage): implement PostgreSQL schema initialization

Create host_events table, host_entries_current view (with IGNORE NULLS),
host_entries_history view, and snapshots table.

Refs #113"
```

---

## Task 4: Implement EventStore Trait

**Files:**
- Create: `crates/router-hosts-storage/src/backends/postgres/event_store.rs`

**Step 1: Write event_store.rs**

```rust
//! EventStore implementation for PostgreSQL
//!
//! Provides append-only event storage with optimistic concurrency control.
//! All operations use transactions for atomicity.

use chrono::Utc;
use sqlx::Row;
use ulid::Ulid;

use super::PostgresStorage;
use crate::error::StorageError;
use crate::types::{EventEnvelope, HostEvent};

/// Helper struct for extracting event data from database rows
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize, Default)]
struct EventData {
    #[serde(skip_serializing_if = "Option::is_none")]
    comment: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    tags: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    previous_ip: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    previous_hostname: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    previous_comment: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    previous_tags: Option<Vec<String>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    deleted_reason: Option<String>,
}

/// Extracted event data for database operations
struct ExtractedEventData {
    ip_address: Option<String>,
    hostname: Option<String>,
    comment: Option<String>,
    tags: Option<String>,
    event_timestamp: chrono::DateTime<Utc>,
    metadata_json: String,
}

impl PostgresStorage {
    /// Append a single event
    pub(crate) async fn append_event_impl(
        &self,
        aggregate_id: Ulid,
        event: EventEnvelope,
        expected_version: Option<String>,
    ) -> Result<(), StorageError> {
        let mut tx = self
            .pool
            .begin()
            .await
            .map_err(|e| StorageError::query("failed to begin transaction", e))?;

        // Check for duplicate on HostCreated
        if let HostEvent::HostCreated {
            ref ip_address,
            ref hostname,
            ..
        } = event.event
        {
            let exists: bool = sqlx::query_scalar(
                "SELECT EXISTS(SELECT 1 FROM host_entries_current WHERE ip_address = $1 AND hostname = $2)",
            )
            .bind(ip_address)
            .bind(hostname)
            .fetch_one(&mut *tx)
            .await
            .map_err(|e| StorageError::query("duplicate check failed", e))?;

            if exists {
                return Err(StorageError::DuplicateEntry {
                    ip: ip_address.clone(),
                    hostname: hostname.clone(),
                });
            }
        }

        // Version check
        let current_version: Option<String> = sqlx::query_scalar(
            "SELECT event_version FROM host_events WHERE aggregate_id = $1 ORDER BY event_version DESC LIMIT 1",
        )
        .bind(aggregate_id.to_string())
        .fetch_optional(&mut *tx)
        .await
        .map_err(|e| StorageError::query("version check failed", e))?;

        if expected_version != current_version {
            return Err(StorageError::ConcurrentWriteConflict {
                aggregate_id: aggregate_id.to_string(),
            });
        }

        // Extract event data
        let extracted = extract_event_data(&event.event)?;

        // Insert event
        sqlx::query(
            r#"
            INSERT INTO host_events (
                event_id, aggregate_id, event_type, event_version,
                ip_address, hostname, comment, tags,
                event_timestamp, metadata,
                created_at, created_by, expected_version
            ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
            "#,
        )
        .bind(event.event_id.to_string())
        .bind(aggregate_id.to_string())
        .bind(event.event.event_type())
        .bind(event.event_version.clone())
        .bind(&extracted.ip_address)
        .bind(&extracted.hostname)
        .bind(&extracted.comment)
        .bind(&extracted.tags)
        .bind(extracted.event_timestamp)
        .bind(&extracted.metadata_json)
        .bind(event.created_at)
        .bind(&event.created_by)
        .bind(&expected_version)
        .execute(&mut *tx)
        .await
        .map_err(|e| {
            let msg = e.to_string();
            if msg.contains("unique") || msg.contains("duplicate") {
                StorageError::ConcurrentWriteConflict {
                    aggregate_id: aggregate_id.to_string(),
                }
            } else {
                StorageError::query("insert event failed", e)
            }
        })?;

        tx.commit()
            .await
            .map_err(|e| StorageError::query("commit failed", e))?;

        Ok(())
    }

    /// Append multiple events atomically
    pub(crate) async fn append_events_impl(
        &self,
        aggregate_id: Ulid,
        events: Vec<EventEnvelope>,
        expected_version: Option<String>,
    ) -> Result<(), StorageError> {
        if events.is_empty() {
            return Ok(());
        }

        let mut tx = self
            .pool
            .begin()
            .await
            .map_err(|e| StorageError::query("failed to begin transaction", e))?;

        // Version check
        let current_version: Option<String> = sqlx::query_scalar(
            "SELECT event_version FROM host_events WHERE aggregate_id = $1 ORDER BY event_version DESC LIMIT 1",
        )
        .bind(aggregate_id.to_string())
        .fetch_optional(&mut *tx)
        .await
        .map_err(|e| StorageError::query("version check failed", e))?;

        if expected_version != current_version {
            return Err(StorageError::ConcurrentWriteConflict {
                aggregate_id: aggregate_id.to_string(),
            });
        }

        // Check for duplicates on any HostCreated events
        for event in &events {
            if let HostEvent::HostCreated {
                ref ip_address,
                ref hostname,
                ..
            } = event.event
            {
                let exists: bool = sqlx::query_scalar(
                    "SELECT EXISTS(SELECT 1 FROM host_entries_current WHERE ip_address = $1 AND hostname = $2)",
                )
                .bind(ip_address)
                .bind(hostname)
                .fetch_one(&mut *tx)
                .await
                .map_err(|e| StorageError::query("duplicate check failed", e))?;

                if exists {
                    return Err(StorageError::DuplicateEntry {
                        ip: ip_address.clone(),
                        hostname: hostname.clone(),
                    });
                }
            }
        }

        // Insert all events
        for event in events {
            let extracted = extract_event_data(&event.event)?;

            sqlx::query(
                r#"
                INSERT INTO host_events (
                    event_id, aggregate_id, event_type, event_version,
                    ip_address, hostname, comment, tags,
                    event_timestamp, metadata,
                    created_at, created_by, expected_version
                ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
                "#,
            )
            .bind(event.event_id.to_string())
            .bind(aggregate_id.to_string())
            .bind(event.event.event_type())
            .bind(event.event_version.clone())
            .bind(&extracted.ip_address)
            .bind(&extracted.hostname)
            .bind(&extracted.comment)
            .bind(&extracted.tags)
            .bind(extracted.event_timestamp)
            .bind(&extracted.metadata_json)
            .bind(event.created_at)
            .bind(&event.created_by)
            .bind(&expected_version)
            .execute(&mut *tx)
            .await
            .map_err(|e| StorageError::query("insert event failed", e))?;
        }

        tx.commit()
            .await
            .map_err(|e| StorageError::query("commit failed", e))?;

        Ok(())
    }

    /// Load all events for an aggregate
    pub(crate) async fn load_events_impl(
        &self,
        aggregate_id: Ulid,
    ) -> Result<Vec<EventEnvelope>, StorageError> {
        let rows = sqlx::query(
            r#"
            SELECT
                event_id, aggregate_id, event_type, event_version,
                ip_address, hostname, metadata, event_timestamp,
                created_at, created_by
            FROM host_events
            WHERE aggregate_id = $1
            ORDER BY event_version ASC
            "#,
        )
        .bind(aggregate_id.to_string())
        .fetch_all(self.pool())
        .await
        .map_err(|e| StorageError::query("load_events failed", e))?;

        let mut envelopes = Vec::with_capacity(rows.len());

        for row in rows {
            let event_id_str: String = row.get("event_id");
            let event_type: String = row.get("event_type");
            let event_version: String = row.get("event_version");
            let ip_address: Option<String> = row.get("ip_address");
            let hostname: Option<String> = row.get("hostname");
            let metadata_json: String = row.get("metadata");
            let event_timestamp: chrono::DateTime<Utc> = row.get("event_timestamp");
            let created_at: chrono::DateTime<Utc> = row.get("created_at");
            let created_by: Option<String> = row.get("created_by");

            let event_id = Ulid::from_string(&event_id_str)
                .map_err(|e| StorageError::InvalidData(format!("invalid event_id: {}", e)))?;

            let event_data: EventData = serde_json::from_str(&metadata_json)
                .map_err(|e| StorageError::InvalidData(format!("JSON parse failed: {}", e)))?;

            let event =
                reconstruct_event(&event_type, ip_address, hostname, event_timestamp, &event_data)?;

            envelopes.push(EventEnvelope {
                event_id,
                aggregate_id,
                event,
                event_version,
                created_at,
                created_by,
            });
        }

        Ok(envelopes)
    }

    /// Get current version for an aggregate
    pub(crate) async fn get_current_version_impl(
        &self,
        aggregate_id: Ulid,
    ) -> Result<Option<String>, StorageError> {
        let version: Option<String> = sqlx::query_scalar(
            "SELECT event_version FROM host_events WHERE aggregate_id = $1 ORDER BY event_version DESC LIMIT 1",
        )
        .bind(aggregate_id.to_string())
        .fetch_optional(self.pool())
        .await
        .map_err(|e| StorageError::query("get_current_version failed", e))?;

        Ok(version)
    }

    /// Count events for an aggregate
    pub(crate) async fn count_events_impl(&self, aggregate_id: Ulid) -> Result<i64, StorageError> {
        let count: i64 = sqlx::query_scalar(
            "SELECT COUNT(*) FROM host_events WHERE aggregate_id = $1",
        )
        .bind(aggregate_id.to_string())
        .fetch_one(self.pool())
        .await
        .map_err(|e| StorageError::query("count_events failed", e))?;

        Ok(count)
    }
}

/// Extract column values and metadata JSON from an event
fn extract_event_data(event: &HostEvent) -> Result<ExtractedEventData, StorageError> {
    let (ip_address, hostname, comment, tags, event_timestamp, event_data) = match event {
        HostEvent::HostCreated {
            ip_address,
            hostname,
            comment,
            tags,
            created_at,
        } => (
            Some(ip_address.clone()),
            Some(hostname.clone()),
            comment.clone(),
            Some(serde_json::to_string(tags).unwrap_or_else(|_| "[]".into())),
            *created_at,
            EventData {
                comment: comment.clone(),
                tags: Some(tags.clone()),
                ..Default::default()
            },
        ),
        HostEvent::IpAddressChanged {
            old_ip,
            new_ip,
            changed_at,
        } => (
            Some(new_ip.clone()),
            None,
            None,
            None,
            *changed_at,
            EventData {
                previous_ip: Some(old_ip.clone()),
                ..Default::default()
            },
        ),
        HostEvent::HostnameChanged {
            old_hostname,
            new_hostname,
            changed_at,
        } => (
            None,
            Some(new_hostname.clone()),
            None,
            None,
            *changed_at,
            EventData {
                previous_hostname: Some(old_hostname.clone()),
                ..Default::default()
            },
        ),
        HostEvent::CommentUpdated {
            old_comment,
            new_comment,
            updated_at,
        } => (
            None,
            None,
            new_comment.clone(),
            None,
            *updated_at,
            EventData {
                comment: new_comment.clone(),
                previous_comment: old_comment.clone(),
                ..Default::default()
            },
        ),
        HostEvent::TagsModified {
            old_tags,
            new_tags,
            modified_at,
        } => (
            None,
            None,
            None,
            Some(serde_json::to_string(new_tags).unwrap_or_else(|_| "[]".into())),
            *modified_at,
            EventData {
                tags: Some(new_tags.clone()),
                previous_tags: Some(old_tags.clone()),
                ..Default::default()
            },
        ),
        HostEvent::HostDeleted {
            ip_address,
            hostname,
            deleted_at,
            reason,
        } => (
            Some(ip_address.clone()),
            Some(hostname.clone()),
            None,
            None,
            *deleted_at,
            EventData {
                deleted_reason: reason.clone(),
                ..Default::default()
            },
        ),
    };

    let metadata_json = serde_json::to_string(&event_data)
        .map_err(|e| StorageError::InvalidData(format!("JSON serialization failed: {}", e)))?;

    Ok(ExtractedEventData {
        ip_address,
        hostname,
        comment,
        tags,
        event_timestamp,
        metadata_json,
    })
}

/// Reconstruct a HostEvent from database columns
fn reconstruct_event(
    event_type: &str,
    ip: Option<String>,
    hostname: Option<String>,
    event_ts: chrono::DateTime<Utc>,
    data: &EventData,
) -> Result<HostEvent, StorageError> {
    match event_type {
        "HostCreated" => Ok(HostEvent::HostCreated {
            ip_address: ip.ok_or_else(|| StorageError::InvalidData("missing ip".into()))?,
            hostname: hostname
                .ok_or_else(|| StorageError::InvalidData("missing hostname".into()))?,
            comment: data.comment.clone(),
            tags: data.tags.clone().unwrap_or_default(),
            created_at: event_ts,
        }),
        "IpAddressChanged" => Ok(HostEvent::IpAddressChanged {
            old_ip: data
                .previous_ip
                .clone()
                .ok_or_else(|| StorageError::InvalidData("missing previous_ip".into()))?,
            new_ip: ip.ok_or_else(|| StorageError::InvalidData("missing ip".into()))?,
            changed_at: event_ts,
        }),
        "HostnameChanged" => Ok(HostEvent::HostnameChanged {
            old_hostname: data
                .previous_hostname
                .clone()
                .ok_or_else(|| StorageError::InvalidData("missing previous_hostname".into()))?,
            new_hostname: hostname
                .ok_or_else(|| StorageError::InvalidData("missing hostname".into()))?,
            changed_at: event_ts,
        }),
        "CommentUpdated" => Ok(HostEvent::CommentUpdated {
            old_comment: data.previous_comment.clone(),
            new_comment: data.comment.clone(),
            updated_at: event_ts,
        }),
        "TagsModified" => Ok(HostEvent::TagsModified {
            old_tags: data.previous_tags.clone().unwrap_or_default(),
            new_tags: data.tags.clone().unwrap_or_default(),
            modified_at: event_ts,
        }),
        "HostDeleted" => Ok(HostEvent::HostDeleted {
            ip_address: ip.ok_or_else(|| StorageError::InvalidData("missing ip".into()))?,
            hostname: hostname
                .ok_or_else(|| StorageError::InvalidData("missing hostname".into()))?,
            deleted_at: event_ts,
            reason: data.deleted_reason.clone(),
        }),
        _ => Err(StorageError::InvalidData(format!(
            "unknown event type: {}",
            event_type
        ))),
    }
}
```

**Step 2: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Errors about missing snapshot_store, projection

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/src/backends/postgres/event_store.rs
git commit -m "feat(storage): implement PostgreSQL EventStore

Add event appending with optimistic concurrency, duplicate detection,
and event reconstruction from database rows.

Refs #113"
```

---

## Task 5: Implement SnapshotStore Trait

**Files:**
- Create: `crates/router-hosts-storage/src/backends/postgres/snapshot_store.rs`

**Step 1: Write snapshot_store.rs**

```rust
//! SnapshotStore implementation for PostgreSQL
//!
//! Provides versioned /etc/hosts snapshots with retention policies.

use chrono::Utc;
use sqlx::Row;

use super::PostgresStorage;
use crate::error::StorageError;
use crate::types::{Snapshot, SnapshotId, SnapshotMetadata};

impl PostgresStorage {
    /// Save a snapshot
    pub(crate) async fn save_snapshot_impl(&self, snapshot: Snapshot) -> Result<(), StorageError> {
        sqlx::query(
            r#"
            INSERT INTO snapshots (
                snapshot_id, created_at, hosts_content,
                entry_count, trigger, name, event_log_position
            ) VALUES ($1, $2, $3, $4, $5, $6, $7)
            ON CONFLICT (snapshot_id) DO UPDATE SET
                hosts_content = EXCLUDED.hosts_content,
                entry_count = EXCLUDED.entry_count,
                trigger = EXCLUDED.trigger,
                name = EXCLUDED.name,
                event_log_position = EXCLUDED.event_log_position
            "#,
        )
        .bind(snapshot.id.as_str())
        .bind(snapshot.created_at)
        .bind(&snapshot.hosts_content)
        .bind(snapshot.entry_count)
        .bind(&snapshot.trigger)
        .bind(&snapshot.name)
        .bind(snapshot.event_log_position)
        .execute(self.pool())
        .await
        .map_err(|e| StorageError::query("save_snapshot failed", e))?;

        Ok(())
    }

    /// Get a snapshot by ID
    pub(crate) async fn get_snapshot_impl(
        &self,
        snapshot_id: &SnapshotId,
    ) -> Result<Snapshot, StorageError> {
        let row = sqlx::query(
            r#"
            SELECT snapshot_id, created_at, hosts_content,
                   entry_count, trigger, name, event_log_position
            FROM snapshots
            WHERE snapshot_id = $1
            "#,
        )
        .bind(snapshot_id.as_str())
        .fetch_optional(self.pool())
        .await
        .map_err(|e| StorageError::query("get_snapshot failed", e))?
        .ok_or_else(|| StorageError::NotFound {
            entity_type: "Snapshot",
            id: snapshot_id.to_string(),
        })?;

        Ok(Snapshot {
            id: SnapshotId::new(row.get::<String, _>("snapshot_id")),
            created_at: row.get("created_at"),
            hosts_content: row.get("hosts_content"),
            entry_count: row.get("entry_count"),
            trigger: row.get("trigger"),
            name: row.get("name"),
            event_log_position: row.get("event_log_position"),
        })
    }

    /// List snapshots with pagination
    pub(crate) async fn list_snapshots_impl(
        &self,
        limit: Option<u32>,
        offset: Option<u32>,
    ) -> Result<Vec<SnapshotMetadata>, StorageError> {
        let limit = limit.unwrap_or(100) as i64;
        let offset = offset.unwrap_or(0) as i64;

        let rows = sqlx::query(
            r#"
            SELECT snapshot_id, created_at, entry_count, trigger, name
            FROM snapshots
            ORDER BY created_at DESC
            LIMIT $1 OFFSET $2
            "#,
        )
        .bind(limit)
        .bind(offset)
        .fetch_all(self.pool())
        .await
        .map_err(|e| StorageError::query("list_snapshots failed", e))?;

        let snapshots = rows
            .into_iter()
            .map(|row| SnapshotMetadata {
                id: SnapshotId::new(row.get::<String, _>("snapshot_id")),
                created_at: row.get("created_at"),
                entry_count: row.get("entry_count"),
                trigger: row.get("trigger"),
                name: row.get("name"),
            })
            .collect();

        Ok(snapshots)
    }

    /// Delete a snapshot by ID
    pub(crate) async fn delete_snapshot_impl(
        &self,
        snapshot_id: &SnapshotId,
    ) -> Result<(), StorageError> {
        let result = sqlx::query("DELETE FROM snapshots WHERE snapshot_id = $1")
            .bind(snapshot_id.as_str())
            .execute(self.pool())
            .await
            .map_err(|e| StorageError::query("delete_snapshot failed", e))?;

        if result.rows_affected() == 0 {
            return Err(StorageError::NotFound {
                entity_type: "Snapshot",
                id: snapshot_id.to_string(),
            });
        }

        Ok(())
    }

    /// Apply retention policy
    pub(crate) async fn apply_retention_policy_impl(
        &self,
        max_count: Option<usize>,
        max_age_days: Option<u32>,
    ) -> Result<usize, StorageError> {
        let mut deleted = 0usize;

        // Delete by age first
        if let Some(max_age) = max_age_days {
            let cutoff = Utc::now() - chrono::Duration::days(max_age as i64);

            let result = sqlx::query("DELETE FROM snapshots WHERE created_at < $1")
                .bind(cutoff)
                .execute(self.pool())
                .await
                .map_err(|e| StorageError::query("retention by age failed", e))?;

            deleted += result.rows_affected() as usize;
        }

        // Delete by count
        if let Some(max_count) = max_count {
            let count: i64 = sqlx::query_scalar("SELECT COUNT(*) FROM snapshots")
                .fetch_one(self.pool())
                .await
                .map_err(|e| StorageError::query("count snapshots failed", e))?;

            if count as usize > max_count {
                let to_delete = count as usize - max_count;

                let result = sqlx::query(
                    r#"
                    DELETE FROM snapshots
                    WHERE snapshot_id IN (
                        SELECT snapshot_id FROM snapshots
                        ORDER BY created_at ASC
                        LIMIT $1
                    )
                    "#,
                )
                .bind(to_delete as i64)
                .execute(self.pool())
                .await
                .map_err(|e| StorageError::query("retention by count failed", e))?;

                deleted += result.rows_affected() as usize;
            }
        }

        Ok(deleted)
    }
}
```

**Step 2: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Errors about missing projection

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/src/backends/postgres/snapshot_store.rs
git commit -m "feat(storage): implement PostgreSQL SnapshotStore

Add snapshot CRUD operations and retention policy with age/count limits.

Refs #113"
```

---

## Task 6: Implement HostProjection Trait

**Files:**
- Create: `crates/router-hosts-storage/src/backends/postgres/projection.rs`

**Step 1: Write projection.rs**

```rust
//! HostProjection implementation for PostgreSQL
//!
//! Provides CQRS read-side queries using the host_entries_current view.

use chrono::{DateTime, Utc};
use sqlx::Row;
use ulid::Ulid;

use super::PostgresStorage;
use crate::error::StorageError;
use crate::types::{HostEntry, HostFilter};

impl PostgresStorage {
    /// List all active hosts
    pub(crate) async fn list_all_impl(&self) -> Result<Vec<HostEntry>, StorageError> {
        let rows = sqlx::query(
            r#"
            SELECT id, ip_address, hostname, comment, tags,
                   created_at, updated_at, event_version
            FROM host_entries_current
            ORDER BY ip_address, hostname
            "#,
        )
        .fetch_all(self.pool())
        .await
        .map_err(|e| StorageError::query("list_all failed", e))?;

        rows.into_iter().map(|row| row_to_host_entry(&row)).collect()
    }

    /// Get a host by ID
    pub(crate) async fn get_by_id_impl(&self, id: Ulid) -> Result<HostEntry, StorageError> {
        let row = sqlx::query(
            r#"
            SELECT id, ip_address, hostname, comment, tags,
                   created_at, updated_at, event_version
            FROM host_entries_current
            WHERE id = $1
            "#,
        )
        .bind(id.to_string())
        .fetch_optional(self.pool())
        .await
        .map_err(|e| StorageError::query("get_by_id failed", e))?
        .ok_or_else(|| StorageError::NotFound {
            entity_type: "HostEntry",
            id: id.to_string(),
        })?;

        row_to_host_entry(&row)
    }

    /// Find by IP and hostname
    pub(crate) async fn find_by_ip_and_hostname_impl(
        &self,
        ip_address: &str,
        hostname: &str,
    ) -> Result<Option<HostEntry>, StorageError> {
        let row = sqlx::query(
            r#"
            SELECT id, ip_address, hostname, comment, tags,
                   created_at, updated_at, event_version
            FROM host_entries_current
            WHERE ip_address = $1 AND hostname = $2
            "#,
        )
        .bind(ip_address)
        .bind(hostname)
        .fetch_optional(self.pool())
        .await
        .map_err(|e| StorageError::query("find_by_ip_and_hostname failed", e))?;

        match row {
            Some(r) => Ok(Some(row_to_host_entry(&r)?)),
            None => Ok(None),
        }
    }

    /// Search with filters
    pub(crate) async fn search_impl(
        &self,
        filter: HostFilter,
    ) -> Result<Vec<HostEntry>, StorageError> {
        // Build dynamic query based on filters
        let mut conditions = Vec::new();
        let mut params: Vec<String> = Vec::new();

        if let Some(ref ip_pattern) = filter.ip_pattern {
            params.push(format!("%{}%", ip_pattern));
            conditions.push(format!("ip_address LIKE ${}", params.len()));
        }

        if let Some(ref hostname_pattern) = filter.hostname_pattern {
            params.push(format!("%{}%", hostname_pattern));
            conditions.push(format!("hostname LIKE ${}", params.len()));
        }

        if let Some(ref tags) = filter.tags {
            if !tags.is_empty() {
                // Check if any tag matches
                for tag in tags {
                    params.push(format!("%\"{}%", tag));
                    conditions.push(format!("tags LIKE ${}", params.len()));
                }
            }
        }

        let where_clause = if conditions.is_empty() {
            String::new()
        } else {
            format!("WHERE {}", conditions.join(" AND "))
        };

        let query = format!(
            r#"
            SELECT id, ip_address, hostname, comment, tags,
                   created_at, updated_at, event_version
            FROM host_entries_current
            {}
            ORDER BY ip_address, hostname
            "#,
            where_clause
        );

        // Build query with dynamic parameters
        let mut q = sqlx::query(&query);
        for param in &params {
            q = q.bind(param);
        }

        let rows = q
            .fetch_all(self.pool())
            .await
            .map_err(|e| StorageError::query("search failed", e))?;

        rows.into_iter().map(|row| row_to_host_entry(&row)).collect()
    }

    /// Get state at a specific point in time
    pub(crate) async fn get_at_time_impl(
        &self,
        at_time: DateTime<Utc>,
    ) -> Result<Vec<HostEntry>, StorageError> {
        // Query events up to the given time and reconstruct state
        let rows = sqlx::query(
            r#"
            WITH events_at_time AS (
                SELECT * FROM host_events
                WHERE created_at <= $1
            ),
            windowed AS (
                SELECT
                    aggregate_id,
                    event_version,
                    event_type,
                    LAST_VALUE(ip_address) IGNORE NULLS OVER w as ip_address,
                    LAST_VALUE(hostname) IGNORE NULLS OVER w as hostname,
                    LAST_VALUE(comment) IGNORE NULLS OVER w as comment,
                    LAST_VALUE(tags) IGNORE NULLS OVER w as tags,
                    FIRST_VALUE(event_timestamp) OVER w as created_at,
                    LAST_VALUE(created_at) OVER w as updated_at,
                    LAST_VALUE(event_type) OVER w as latest_event_type,
                    ROW_NUMBER() OVER (PARTITION BY aggregate_id ORDER BY event_version DESC) as rn
                FROM events_at_time
                WINDOW w AS (PARTITION BY aggregate_id ORDER BY event_version
                             ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
            )
            SELECT
                aggregate_id as id,
                ip_address,
                hostname,
                comment,
                tags,
                created_at,
                updated_at,
                event_version
            FROM windowed
            WHERE rn = 1 AND latest_event_type != 'HostDeleted'
            ORDER BY ip_address, hostname
            "#,
        )
        .bind(at_time)
        .fetch_all(self.pool())
        .await
        .map_err(|e| StorageError::query("get_at_time failed", e))?;

        rows.into_iter().map(|row| row_to_host_entry(&row)).collect()
    }
}

/// Convert a database row to HostEntry
fn row_to_host_entry(row: &sqlx::postgres::PgRow) -> Result<HostEntry, StorageError> {
    let id_str: String = row.get("id");
    let id = Ulid::from_string(&id_str)
        .map_err(|e| StorageError::InvalidData(format!("invalid id: {}", e)))?;

    let tags_json: Option<String> = row.get("tags");
    let tags: Vec<String> = tags_json
        .map(|s| serde_json::from_str(&s).unwrap_or_default())
        .unwrap_or_default();

    Ok(HostEntry {
        id,
        ip_address: row.get("ip_address"),
        hostname: row.get("hostname"),
        comment: row.get("comment"),
        tags,
        created_at: row.get("created_at"),
        updated_at: row.get("updated_at"),
        version: row.get("event_version"),
    })
}
```

**Step 2: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Errors about trait implementations not connected

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/src/backends/postgres/projection.rs
git commit -m "feat(storage): implement PostgreSQL HostProjection

Add list_all, get_by_id, find_by_ip_and_hostname, search, and get_at_time
queries using the host_entries_current view.

Refs #113"
```

---

## Task 7: Complete Trait Implementations

**Files:**
- Modify: `crates/router-hosts-storage/src/backends/postgres/mod.rs`

**Step 1: Add trait impl blocks**

Add after the `PostgresStorage` impl block (before the `#[cfg(test)]` section if any):

```rust
#[async_trait]
impl EventStore for PostgresStorage {
    async fn append_event(
        &self,
        aggregate_id: Ulid,
        event: EventEnvelope,
        expected_version: Option<String>,
    ) -> Result<(), StorageError> {
        self.append_event_impl(aggregate_id, event, expected_version)
            .await
    }

    async fn append_events(
        &self,
        aggregate_id: Ulid,
        events: Vec<EventEnvelope>,
        expected_version: Option<String>,
    ) -> Result<(), StorageError> {
        self.append_events_impl(aggregate_id, events, expected_version)
            .await
    }

    async fn load_events(&self, aggregate_id: Ulid) -> Result<Vec<EventEnvelope>, StorageError> {
        self.load_events_impl(aggregate_id).await
    }

    async fn get_current_version(
        &self,
        aggregate_id: Ulid,
    ) -> Result<Option<String>, StorageError> {
        self.get_current_version_impl(aggregate_id).await
    }

    async fn count_events(&self, aggregate_id: Ulid) -> Result<i64, StorageError> {
        self.count_events_impl(aggregate_id).await
    }
}

#[async_trait]
impl SnapshotStore for PostgresStorage {
    async fn save_snapshot(&self, snapshot: Snapshot) -> Result<(), StorageError> {
        self.save_snapshot_impl(snapshot).await
    }

    async fn get_snapshot(&self, snapshot_id: &SnapshotId) -> Result<Snapshot, StorageError> {
        self.get_snapshot_impl(snapshot_id).await
    }

    async fn list_snapshots(
        &self,
        limit: Option<u32>,
        offset: Option<u32>,
    ) -> Result<Vec<SnapshotMetadata>, StorageError> {
        self.list_snapshots_impl(limit, offset).await
    }

    async fn delete_snapshot(&self, snapshot_id: &SnapshotId) -> Result<(), StorageError> {
        self.delete_snapshot_impl(snapshot_id).await
    }

    async fn apply_retention_policy(
        &self,
        max_count: Option<usize>,
        max_age_days: Option<u32>,
    ) -> Result<usize, StorageError> {
        self.apply_retention_policy_impl(max_count, max_age_days)
            .await
    }
}

#[async_trait]
impl HostProjection for PostgresStorage {
    async fn list_all(&self) -> Result<Vec<HostEntry>, StorageError> {
        self.list_all_impl().await
    }

    async fn get_by_id(&self, id: Ulid) -> Result<HostEntry, StorageError> {
        self.get_by_id_impl(id).await
    }

    async fn find_by_ip_and_hostname(
        &self,
        ip_address: &str,
        hostname: &str,
    ) -> Result<Option<HostEntry>, StorageError> {
        self.find_by_ip_and_hostname_impl(ip_address, hostname)
            .await
    }

    async fn search(&self, filter: HostFilter) -> Result<Vec<HostEntry>, StorageError> {
        self.search_impl(filter).await
    }

    async fn get_at_time(&self, at_time: DateTime<Utc>) -> Result<Vec<HostEntry>, StorageError> {
        self.get_at_time_impl(at_time).await
    }
}

#[async_trait]
impl Storage for PostgresStorage {
    async fn initialize(&self) -> Result<(), StorageError> {
        schema::initialize_schema(self).await
    }

    async fn health_check(&self) -> Result<(), StorageError> {
        sqlx::query("SELECT 1")
            .execute(self.pool())
            .await
            .map_err(|e| StorageError::connection("health check failed", e))?;
        Ok(())
    }

    async fn close(&self) -> Result<(), StorageError> {
        self.pool.close().await;
        Ok(())
    }
}
```

**Step 2: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Compiles successfully

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/src/backends/postgres/mod.rs
git commit -m "feat(storage): complete PostgreSQL Storage trait impls

Wire up EventStore, SnapshotStore, HostProjection, and Storage traits
to their implementation methods.

Refs #113"
```

---

## Task 8: Update lib.rs for PostgreSQL

**Files:**
- Modify: `crates/router-hosts-storage/src/lib.rs`

**Step 1: Update create_storage function**

Replace the existing `BackendType::Postgres` arm:

```rust
#[cfg(feature = "postgres")]
BackendType::Postgres => std::sync::Arc::new(
    backends::postgres::PostgresStorage::new(&config.connection_string).await?,
),
#[cfg(not(feature = "postgres"))]
BackendType::Postgres => {
    return Err(StorageError::InvalidConnectionString(
        "PostgreSQL backend not compiled in (enable 'postgres' feature)".into(),
    ))
}
```

**Step 2: Update module docs**

Change line 9 from:
```rust
//! - **PostgreSQL** (feature: `postgres`) - Planned for future releases
```
To:
```rust
//! - **PostgreSQL** (feature: `postgres`) - Scalable networked database for multi-instance deployments
```

**Step 3: Verify it compiles**

Run: `cargo check -p router-hosts-storage --features postgres`
Expected: Compiles successfully

**Step 4: Commit**

```bash
git add crates/router-hosts-storage/src/lib.rs
git commit -m "feat(storage): wire PostgreSQL backend into create_storage

Update create_storage to instantiate PostgresStorage when postgres feature
is enabled. Update docs to reflect PostgreSQL is now implemented.

Refs #113"
```

---

## Task 9: Create Integration Test

**Files:**
- Create: `crates/router-hosts-storage/tests/postgres_backend.rs`

**Step 1: Write test file**

```rust
//! PostgreSQL backend integration tests
//!
//! Uses testcontainers to spin up a PostgreSQL instance for testing.
//! All tests use the shared test harness from tests/common/.

mod common;

#[cfg(feature = "postgres")]
mod postgres_tests {
    use super::common;
    use router_hosts_storage::backends::postgres::PostgresStorage;
    use router_hosts_storage::Storage;
    use testcontainers::{runners::AsyncRunner, ContainerAsync};
    use testcontainers_modules::postgres::Postgres;

    /// Set up a PostgreSQL container and storage instance
    async fn setup_postgres() -> (ContainerAsync<Postgres>, PostgresStorage) {
        let container = Postgres::default()
            .start()
            .await
            .expect("Failed to start PostgreSQL container");

        let port = container
            .get_host_port_ipv4(5432)
            .await
            .expect("Failed to get PostgreSQL port");

        let url = format!("postgres://postgres:postgres@127.0.0.1:{}/postgres", port);

        let storage = PostgresStorage::new(&url)
            .await
            .expect("Failed to create PostgreSQL storage");

        storage
            .initialize()
            .await
            .expect("Failed to initialize schema");

        (container, storage)
    }

    #[tokio::test]
    async fn postgres_passes_event_store_tests() {
        let (_container, storage) = setup_postgres().await;
        common::run_event_store_tests(&storage).await;
    }

    #[tokio::test]
    async fn postgres_passes_snapshot_store_tests() {
        let (_container, storage) = setup_postgres().await;
        common::run_snapshot_store_tests(&storage).await;
    }

    #[tokio::test]
    async fn postgres_passes_host_projection_tests() {
        let (_container, storage) = setup_postgres().await;
        common::run_host_projection_tests(&storage).await;
    }

    #[tokio::test]
    async fn postgres_passes_all_tests() {
        let (_container, storage) = setup_postgres().await;
        common::run_all_tests(&storage).await;
    }
}
```

**Step 2: Run tests (requires Docker)**

Run: `cargo test -p router-hosts-storage --features postgres --test postgres_backend -- --nocapture`
Expected: All tests pass (may take 30-60s for container startup)

**Step 3: Commit**

```bash
git add crates/router-hosts-storage/tests/postgres_backend.rs
git commit -m "test(storage): add PostgreSQL integration tests

Use testcontainers to spin up PostgreSQL for testing.
Run full shared test suite against PostgreSQL backend.

Refs #113"
```

---

## Task 10: Update CI Workflow

**Files:**
- Modify: `.github/workflows/ci.yml`

**Step 1: Add PostgreSQL test job**

Add after the existing test job:

```yaml
  test-postgres:
    name: PostgreSQL Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install Rust toolchain
        uses: dtolnay/rust-action@stable
      - name: Run PostgreSQL tests
        run: cargo test -p router-hosts-storage --features postgres --test postgres_backend -- --nocapture
```

**Step 2: Commit**

```bash
git add .github/workflows/ci.yml
git commit -m "ci: add PostgreSQL backend tests to CI

Run PostgreSQL integration tests using testcontainers in CI.

Refs #113"
```

---

## Task 11: Final Verification

**Step 1: Run all tests**

```bash
cargo test --workspace
cargo test -p router-hosts-storage --features postgres --test postgres_backend
```

**Step 2: Run clippy**

```bash
cargo clippy --workspace --all-features -- -D warnings
```

**Step 3: Format check**

```bash
cargo fmt -- --check
```

**Step 4: Create PR**

```bash
git push -u origin feat/postgres-backend
gh pr create --title "feat(storage): add PostgreSQL backend" --body "$(cat <<'EOF'
## Summary
- Add PostgreSQL storage backend using sqlx with connection pooling
- Implement all Storage traits (EventStore, SnapshotStore, HostProjection)
- Use testcontainers for integration testing
- Add postgres feature flag for conditional compilation

## Technical Details
- sqlx 0.8 with rustls for TLS
- Connection pool: min=1, max=10, acquire=30s, idle=10min
- Window functions with IGNORE NULLS (like DuckDB)
- All 42 shared tests passing

## Test Plan
- [x] All existing tests pass
- [x] PostgreSQL integration tests pass
- [x] Clippy clean
- [x] CI workflow updated

Closes #113

 Generated with [Claude Code](https://claude.com/claude-code)
EOF
)"
```

---

## Summary

| Task | Description | Files |
|------|-------------|-------|
| 1 | Add dependencies | Cargo.toml |
| 2 | Create module structure | postgres/mod.rs, backends/mod.rs |
| 3 | Schema initialization | postgres/schema.rs |
| 4 | EventStore impl | postgres/event_store.rs |
| 5 | SnapshotStore impl | postgres/snapshot_store.rs |
| 6 | HostProjection impl | postgres/projection.rs |
| 7 | Wire up trait impls | postgres/mod.rs |
| 8 | Update lib.rs | lib.rs |
| 9 | Integration tests | tests/postgres_backend.rs |
| 10 | CI workflow | ci.yml |
| 11 | Final verification | - |
